INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 76a7d65c5adb9a5e
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6026
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nm/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/neb/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nj/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mon/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nd/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ver/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/res/1.html> (referer: https://anonposted.com/msp/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/177.html> (referer: https://anonposted.com/nh/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/18.html> (referer: https://anonposted.com/nh/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/res/2.html> (referer: https://anonposted.com/min/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/19.html> (referer: https://anonposted.com/nh/catalog.html)
INFO:root:Error parsing thread information. URL: https://anonposted.com/msp/res/1.html
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/28.html> (referer: https://anonposted.com/mai/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/6.html> (referer: https://anonposted.com/mai/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/30.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/2.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/8.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/res/28.html> (referer: https://anonposted.com/mich/catalog.html)
INFO:root:Error parsing thread information. URL: https://anonposted.com/nh/res/177.html
INFO:root:Error parsing thread information. URL: https://anonposted.com/nh/res/18.html
INFO:root:Error parsing thread information. URL: https://anonposted.com/min/res/2.html
INFO:root:Error parsing thread information. URL: https://anonposted.com/nh/res/19.html
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:scrapy.crawler:Received SIGINT twice, forcing unclean shutdown
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 5ba807b4c902d0ba
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6027
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mon/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mon/res/1.html> (referer: https://anonposted.com/mon/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/res/1.html> (referer: https://anonposted.com/msp/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/res/3.html> (referer: https://anonposted.com/msp/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/res/1.html> (referer: https://anonposted.com/min/catalog.html)
INFO:root:Error parsing thread information. URL: https://anonposted.com/mon/res/1.html
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/262.html> (referer: https://anonposted.com/mai/catalog.html)
DEBUG:scrapy.core.scraper:Scraped from <200 https://anonposted.com/mon/res/1.html>
None
INFO:root:Error parsing thread information. URL: https://anonposted.com/msp/res/1.html
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/110.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/89.html> (referer: https://anonposted.com/indi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/163.html> (referer: https://anonposted.com/indi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/364.html> (referer: https://anonposted.com/mai/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/res/2.html> (referer: https://anonposted.com/min/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/69.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/res/22.html> (referer: https://anonposted.com/mary/catalog.html)
INFO:root:Error parsing thread information. URL: https://anonposted.com/msp/res/3.html
INFO:root:Error parsing thread information. URL: https://anonposted.com/min/res/1.html
INFO:root:Error parsing thread information. URL: https://anonposted.com/mai/res/262.html
DEBUG:scrapy.core.scraper:Scraped from <200 https://anonposted.com/msp/res/1.html>
None
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/258.html> (referer: https://anonposted.com/indi/catalog.html)
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: d3ef5b35b0713243
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/col/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ark/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ariz/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/col/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/cal/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ark/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ariz/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ala/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/alb/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ver/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/vir/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ala/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/alb/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ver/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/vir/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ten/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/penn/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/uth/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/tx/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/sc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ri/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/sd/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/or/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ten/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/penn/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/uth/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/tx/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/sc/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ri/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/or/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nev/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/okl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nm/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/neb/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/nev/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/okl/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nj/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/nm/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ny/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/oh/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nd/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/nj/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/nh/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ny/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/oh/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/nd/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mon/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/nc/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/msp/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/min/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mon/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/miss/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
Traceback (most recent call last):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1278, in _get_socket
    sock_info = self.sockets.popleft()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/usr/lib/python3/dist-packages/scrapy/core/spidermw.py", line 84, in evaluate_iterable
    for r in iterable:
  File "/usr/lib/python3/dist-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 79, in parse_region
    existing_thread = self.db[collection_name].find_one({"url": url})
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 1319, in find_one
    for result in cursor.limit(-1):
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1207, in next
    if len(self.__data) or self._refresh():
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 1124, in _refresh
    self.__send_message(q)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/cursor.py", line 999, in __send_message
    response = client._run_operation_with_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1368, in _run_operation_with_response
    return self._retryable_read(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1464, in _retryable_read
    with self._slaveok_for_server(read_pref, server, session,
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1309, in _slaveok_for_server
    with self._get_socket(server, session, exhaust=exhaust) as sock_info:
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/mongo_client.py", line 1246, in _get_socket
    with server.get_socket(
  File "/usr/lib/python3.8/contextlib.py", line 113, in __enter__
    return next(self.gen)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1231, in get_socket
    sock_info = self._get_socket(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1281, in _get_socket
    sock_info = self.connect(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 1197, in connect
    sock_info.check_auth(all_credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 793, in check_auth
    self.authenticate(credentials)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 810, in authenticate
    auth.authenticate(credentials, self)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 673, in authenticate
    auth_func(credentials, sock_info)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 593, in _authenticate_default
    return _authenticate_scram(credentials, sock_info, 'SCRAM-SHA-1')
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/auth.py", line 295, in _authenticate_scram
    res = sock_info.command(source, cmd)
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/pool.py", line 683, in command
    return command(self, dbname, spec, slave_ok,
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/network.py", line 159, in command
    helpers._check_command_response(
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/helpers.py", line 164, in _check_command_response
    raise OperationFailure(errmsg, code, response, max_wire_version)
pymongo.errors.OperationFailure: Authentication failed., full error: {'ok': 0.0, 'errmsg': 'Authentication failed.', 'code': 18, 'codeName': 'AuthenticationFailed'}
INFO:scrapy.core.engine:Closing spider (finished)
ERROR:scrapy.core.engine:Scraper close failure
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/pipelines.py", line 12, in close_spider
    self.connection.close()
  File "/home/adam/.local/lib/python3.8/site-packages/pymongo/collection.py", line 3440, in __call__
    raise TypeError("'Collection' object is not callable. If you "
TypeError: 'Collection' object is not callable. If you meant to call the 'close' method on a 'Database' object it is failing because no such method exists.
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 17697,
 'downloader/request_count': 52,
 'downloader/request_method_count/GET': 52,
 'downloader/response_bytes': 318896,
 'downloader/response_count': 52,
 'downloader/response_status_count/200': 52,
 'elapsed_time_seconds': 3.098495,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 7, 20, 18, 50, 37, 209406),
 'log_count/DEBUG': 52,
 'log_count/ERROR': 47,
 'log_count/INFO': 11,
 'memusage/max': 62509056,
 'memusage/startup': 62509056,
 'request_depth_max': 1,
 'response_received_count': 52,
 'scheduler/dequeued': 52,
 'scheduler/dequeued/memory': 52,
 'scheduler/enqueued': 52,
 'scheduler/enqueued/memory': 52,
 'spider_exceptions/OperationFailure': 46,
 'start_time': datetime.datetime(2021, 7, 20, 18, 50, 34, 110911)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 1c089d8dfbbebb22
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/30.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/2.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/res/2.html> (referer: https://anonposted.com/min/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/res/22.html> (referer: https://anonposted.com/mary/catalog.html)
INFO:root:Error parsing thread information. URL: https://anonposted.com/miss/res/30.html
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/13.html> (referer: https://anonposted.com/ken/catalog.html)
INFO:root:Error parsing thread information. URL: https://anonposted.com/miss/res/2.html
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:scrapy.crawler:Received SIGINT twice, forcing unclean shutdown
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 8d80c235718467df
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6024
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/col/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ariz/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/res/2.html> (referer: https://anonposted.com/lou/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/res/1.html> (referer: https://anonposted.com/lou/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/res/5.html> (referer: https://anonposted.com/lou/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/uth/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/vir/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ver/catalog.html> (referer: https://anonposted.com)
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:scrapy.crawler:Received SIGINT twice, forcing unclean shutdown
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 402e5898c4f50871
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6025
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/col/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/res/57.html> (referer: https://anonposted.com/mary/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mary/res/57.html> (referer: https://anonposted.com/mary/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/911.html> (referer: https://anonposted.com/indi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/res/2.html> (referer: https://anonposted.com/kan/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/res/9.html> (referer: https://anonposted.com/kan/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/864.html> (referer: https://anonposted.com/indi/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/indi/res/911.html> (referer: https://anonposted.com/indi/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/kan/res/2.html> (referer: https://anonposted.com/kan/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/kan/res/9.html> (referer: https://anonposted.com/kan/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/117.html> (referer: https://anonposted.com/indi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/1700.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/772.html> (referer: https://anonposted.com/indi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/130.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/20.html> (referer: https://anonposted.com/ken/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/indi/res/864.html> (referer: https://anonposted.com/indi/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/69.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/28.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/64.html> (referer: https://anonposted.com/ken/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/indi/res/117.html> (referer: https://anonposted.com/indi/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/1700.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/indi/res/772.html> (referer: https://anonposted.com/indi/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/130.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/20.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/69.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/28.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/25.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/13.html> (referer: https://anonposted.com/ken/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/64.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/726.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/73.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/25.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/13.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/24.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/5.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/197.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/726.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/73.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/50.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/24.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/5.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/983.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/31.html> (referer: https://anonposted.com/ken/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/197.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/884.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/50.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/259.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/983.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/31.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/568.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/884.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/131.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/259.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/res/5.html> (referer: https://anonposted.com/con/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/484.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/568.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/310.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/271.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/131.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/54.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/con/res/5.html> (referer: https://anonposted.com/con/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/484.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/4.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/310.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/512.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/271.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/64.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/54.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/55.html> (referer: https://anonposted.com/mas/catalog.html)
INFO:scrapy.core.engine:Closing spider (shutdown)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/280.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/4.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/512.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/52.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/64.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/176.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/251.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/125.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/55.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/280.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/382.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/52.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/209.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/176.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/226.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/251.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/125.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/497.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/104.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/382.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.now(pytz.utc)
AttributeError: module 'datetime' has no attribute 'now'
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: a394afdd386f6604
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6026
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/75.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/res/544.html> (referer: https://anonposted.com/mich/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/1700.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/ken/res/75.html> (referer: https://anonposted.com/ken/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/34.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mich/res/544.html> (referer: https://anonposted.com/mich/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/1700.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/1458.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/1170.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/34.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/1062.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/res/2.html> (referer: https://anonposted.com/lou/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/res/5.html> (referer: https://anonposted.com/lou/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/res/1.html> (referer: https://anonposted.com/haw/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/1458.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/1170.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/res/12.html> (referer: https://anonposted.com/iow/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/1062.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/lou/res/2.html> (referer: https://anonposted.com/lou/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/lou/res/5.html> (referer: https://anonposted.com/lou/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/haw/res/1.html> (referer: https://anonposted.com/haw/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/res/2.html> (referer: https://anonposted.com/iow/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/res/1.html> (referer: https://anonposted.com/iow/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/res/59.html> (referer: https://anonposted.com/iow/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/iow/res/12.html> (referer: https://anonposted.com/iow/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/res/3.html> (referer: https://anonposted.com/iow/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/148.html> (referer: https://anonposted.com/mai/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/iow/res/2.html> (referer: https://anonposted.com/iow/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/iow/res/1.html> (referer: https://anonposted.com/iow/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/iow/res/59.html> (referer: https://anonposted.com/iow/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/262.html> (referer: https://anonposted.com/mai/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/64.html> (referer: https://anonposted.com/mai/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/iow/res/3.html> (referer: https://anonposted.com/iow/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/197.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/624.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mai/res/148.html> (referer: https://anonposted.com/mai/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/25.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mai/res/262.html> (referer: https://anonposted.com/mai/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/res/65.html> (referer: https://anonposted.com/wash/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mai/res/64.html> (referer: https://anonposted.com/mai/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/197.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/624.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/73.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/res/1.html> (referer: https://anonposted.com/wash/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/25.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/res/17.html> (referer: https://anonposted.com/wash/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/566.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/res/65.html> (referer: https://anonposted.com/wash/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/res/16.html> (referer: https://anonposted.com/wash/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/73.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/res/1.html> (referer: https://anonposted.com/wash/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/res/14.html> (referer: https://anonposted.com/wash/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/res/17.html> (referer: https://anonposted.com/wash/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/566.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/res/16.html> (referer: https://anonposted.com/wash/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/145.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/res/5.html> (referer: https://anonposted.com/wash/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/78.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/res/14.html> (referer: https://anonposted.com/wash/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/640.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/res/13.html> (referer: https://anonposted.com/wash/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/75.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/52.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/145.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/res/5.html> (referer: https://anonposted.com/wash/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/78.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/7.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/640.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wash/res/13.html> (referer: https://anonposted.com/wash/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/75.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/602.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/52.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/146.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/71.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/4.html> (referer: https://anonposted.com/wv/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/19.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/7.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/125.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/602.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/res/144.html> (referer: https://anonposted.com/wv/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/146.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/54.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/71.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/4.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/19.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/64.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/125.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/55.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/176.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/280.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/wv/res/144.html> (referer: https://anonposted.com/wv/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/54.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/52.html> (referer: https://anonposted.com/mas/catalog.html)
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/251.html> (referer: https://anonposted.com/mas/catalog.html)
INFO:scrapy.core.engine:Closing spider (shutdown)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/104.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/64.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/55.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/176.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/280.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/209.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/52.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/251.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/226.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/310.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/497.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/104.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/243.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/382.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/360.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/100.html> (referer: https://anonposted.com/mas/catalog.html)
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/209.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/226.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/310.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/mas/res/497.html> (referer: https://anonposted.com/mas/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/persephone/rp_spider/spiders/Anonme.py", line 92, in parse_thread
    thread_item["date_of_last_scrape"] = datetime.datetime.now(pytz.utc)
  File "/usr/lib/python3/dist-packages/scrapy/item.py", line 72, in __setitem__
    raise KeyError("%s does not support field: %s" %
KeyError: 'AnonmeItem does not support field: date_of_last_scrape'
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/336.html> (referer: https://anonposted.com/mas/catalog.html)
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 21d410b7a16d342a
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6027
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/col/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/res/9.html> (referer: https://anonposted.com/kan/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/1170.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/res/2.html> (referer: https://anonposted.com/kan/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/693.html> (referer: https://anonposted.com/mas/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/res/925.html> (referer: https://anonposted.com/indi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/res/1062.html> (referer: https://anonposted.com/mas/catalog.html)
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:scrapy.crawler:Received SIGINT twice, forcing unclean shutdown
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: c60f5b776e039bf6
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6028
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/2.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/18.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/8.html> (referer: https://anonposted.com/miss/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/res/414.html> (referer: https://anonposted.com/mai/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/res/6.html> (referer: https://anonposted.com/miss/catalog.html)
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:scrapy.crawler:Received SIGINT twice, forcing unclean shutdown
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-59-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 543524b4899c07db
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6029
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/msp/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mich/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/miss/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/min/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nd/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/neb/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mon/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/16.html> (referer: https://anonposted.com/nh/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/89.html> (referer: https://anonposted.com/nh/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/44.html> (referer: https://anonposted.com/nh/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/72.html> (referer: https://anonposted.com/nh/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/69.html> (referer: https://anonposted.com/nh/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/nh/res/57.html> (referer: https://anonposted.com/nh/catalog.html)
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-63-generic-x86_64-with-glibc2.29
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-63-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 5a1a3a87ac6cb1bd
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-63-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 1e8d662566236707
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-63-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: e6e004fca4bf219d
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ark/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ariz/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/col/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/alb/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ala/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/res/12.html> (referer: https://anonposted.com/geo/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/res/17.html> (referer: https://anonposted.com/geo/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/vir/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/res/5.html> (referer: https://anonposted.com/geo/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ver/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/res/14.html> (referer: https://anonposted.com/geo/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/res/15.html> (referer: https://anonposted.com/geo/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/res/11.html> (referer: https://anonposted.com/geo/catalog.html)
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:root:Error parsing thread information. URL: https://anonposted.com/geo/res/12.html
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/res/30.html> (referer: https://anonposted.com/cal/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/res/94.html> (referer: https://anonposted.com/cal/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/res/206.html> (referer: https://anonposted.com/cal/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/res/169.html> (referer: https://anonposted.com/cal/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/cal/res/125.html> (referer: https://anonposted.com/cal/catalog.html)
INFO:scrapy.core.engine:Closing spider (shutdown)
INFO:scrapy.crawler:Received SIGINT twice, forcing unclean shutdown
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-63-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 364ecf03db6d6808
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6024
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mai/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/res/154.html> (referer: https://anonposted.com/illi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/res/252.html> (referer: https://anonposted.com/illi/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/res/4.html> (referer: https://anonposted.com/illi/catalog.html)
INFO:root:Error parsing thread information. URL: https://anonposted.com/illi/res/154.html
ERROR:scrapy.core.scraper:Spider error processing <GET https://anonposted.com/illi/res/154.html> (referer: https://anonposted.com/illi/catalog.html)
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/twisted/internet/defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/home/adam/persephone/rp_spider/spiders/Anonme.py", line 209, in parse_thread
    return thread_item
  File "/home/adam/persephone/rp_spider/spiders/Anonme.py", line 209, in parse_thread
    return thread_item
  File "/usr/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/usr/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
INFO:root:Error parsing thread information. URL: https://anonposted.com/illi/res/252.html
INFO:scrapy.crawler:Received SIGINT, shutting down gracefully. Send again to force 
INFO:scrapy.utils.log:Scrapy 1.7.3 started (bot: rp_spider)
INFO:scrapy.utils.log:Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 18.9.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.8.0-63-generic-x86_64-with-glibc2.29
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'rp_spider', 'NEWSPIDER_MODULE': 'rp_spider.spiders', 'SPIDER_MODULES': ['rp_spider.spiders']}
INFO:scrapy.extensions.telnet:Telnet Password: 8aa9c62b26dcabe7
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:root:Sucessfully connected to database within spider
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'random_useragent.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['rp_spider.pipelines.MongoDBPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6025
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mary/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ida/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/kan/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/indi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/illi/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/geo/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/iow/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/lou/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/mas/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/haw/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/fl/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/del/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/dc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/con/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ver/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wisc/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/col/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wy/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wv/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/wash/catalog.html> (referer: https://anonposted.com)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/183.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/21.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/55.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/5.html> (referer: https://anonposted.com/ken/catalog.html)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://anonposted.com/ken/res/130.html> (referer: https://anonposted.com/ken/catalog.html)
